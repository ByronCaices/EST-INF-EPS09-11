---
title: "EP09"
author: "Equipo 2"
date: "2024-12-02"
output:
    html_document:
    highlight: tango
    word_document: default
    pdf_document: default
---

```{=html}
<style>
body {
  font-family: 'Calibri', sans-serif;
}
</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo =FALSE, warning=FALSE, message=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))

if (!requireNamespace('tidyverse', quietly = TRUE)){
  install.packages('tidyverse')
}
library(tidyverse)
if (!requireNamespace('ggpubr', quietly = TRUE)){
  install.packages('ggpubr')
}
library(car)
if (!requireNamespace('car', quietly = TRUE)){
  install.packages('car')
}
library(ggpubr)
if (!requireNamespace('ez', quietly = TRUE)){
  install.packages('ez')
}
library(ez)
if (!requireNamespace('RVAideMemoire', quietly = TRUE)){
  install.packages('RVAideMemoire')
}
library(RVAideMemoire)
if (!requireNamespace('rcompanion', quietly = TRUE)){
  install.packages('rcompanion')
}
library(rcompanion)
if (!requireNamespace('dplyr', quietly = TRUE)){
  install.packages('dplyr')
}
library(dplyr)
if (!requireNamespace('WRS2', quietly = TRUE)){
  install.packages('WRS2')
}
library(WRS2)
```

Primero leemos los datos
```{r leer archivo}
datosGenerales <- read.csv2("EP09 Datos.csv")
head(datos)
```
Luego fijamos una semilla y como la semilla es impar obtenemos una muestra aleatoria de 100 hombres, donde usamos 70 muestras para generar el modelo de regresión simple y múltiple, ademas obtenemos ocho variables aleatoriamente para como posibles predictoras.
```{r muestras}
set.seed(7525)

datos = datosGenerales %>% filter(Gender == 1) %>% sample_n(100)

i_datos = sample.int(n = 100, size = 70, replace = FALSE)

datosModelos = datos[i_datos,]
datosPruebas = datos[-i_datos,]

predictores = sample(colnames(datos), 8)

print(predictores)
```
Primero creamos el modelo simple con la variable "Waist.Girth" que corresponde al grosor de la cintura pues .....
```{r modelo simple}
modeloSimple = lm(Weight ~ Waist.Girth, data = datosModelos)
print(summary(modeloSimple))
```
Empezamos a crear un modelo múltiple en base al modelo simple aplicando la estrategia de eliminación hacia atras, para esto creamos un dataframe solo con las variables obtenidas aleatoriamente y generamos un modelo considerando todas las variables obtenidas como predictoras, para la eliminación de predictores, eliminaremos el predictor que presente un menor valor del estadistico F.
```{r nuevo df}
datosModelos = datosModelos %>% select(Weight, Waist.Girth, Knee.Girth, Chest.diameter, Wrist.Minimum.Girth, Thigh.Girth, Height, Calf.Maximum.Girth)

modeloCompleto = lm(Weight ~ ., data = datosModelos)
print(summary(modeloCompleto))
```
Ahora comenzamos con la regresión paso a paso, donde primero generamos los resultados de los modelos al eliminar un predictor.
```{r modelo multiple 1}
paso = drop1(modeloCompleto, test = "F")
print(paso, digits = 3, signif.legend = TRUE)
```
Observamos que el predictor Knee.Girth tiene un menor valor del estadistico F por lo que lo eliminamos del modelo y hacemos otro paso más de la regresión paso a paso
```{r modelo multiple 2}
modeloMultiple1 = update(modeloCompleto, . ~ . - Knee.Girth)

paso = drop1(modeloMultiple1, test = "F")
print(paso, digits = 3, signif.legend = TRUE)
```
Observamos que el predictor Chest.diameter tiene un menor valor del estadistico F por lo que lo eliminamos del modelo y hacemos otro paso más de la regresión paso a paso
```{r modelo multiple 3}
modeloMultiple2 = update(modeloMultiple1, . ~ . - Chest.diameter)

paso = drop1(modeloMultiple2, test = "F")
print(paso, digits = 3, signif.legend = TRUE)
```

Observamos que el predictor Calf.Maximum.Girth. tiene un menor valor del estadistico F por lo que lo eliminamos del modelo y hacemos otro paso más de la regresión paso a paso, cabe resaltar que si bien tiene un menor valor del estadistico F, tambien posee un valor p muy pequeño por lo cual comparamos el modelo con y sin este predictor.
```{r modelo multiple 4}
modeloMultiple3 = update(modeloMultiple2, . ~ . - Calf.Maximum.Girth)
print(summary(modeloMultiple2))
print(summary(modeloMultiple3))

modeloMultiple = modeloMultiple2
```
Observamos que el modelo con el predictor Calf.Maximum.Girth posee un mayor valor R cuadrado que el modelo sin este predictor por lo cual dejamos el modelo que contiene este predictor.

## Bondad de ajuste de los modelos

Considerando un nivel de significancia  estandar de 0.05 podemos afirmar que:

Con respecto al modelo simple, podemos observar que se obtiene un r cuadrado de 0,685 por lo cual podemos afirmar que el modelo basado en el grosor de la cintura reduce en un 68,5% la varianza aleatoria respecto al modelo nulo, además de que esta reducción es significativa puesto que el valor p obtenido del modelo es mucho menor al nivel de significancia establecido.

Y con respecto al modelo múltiple que considera como predictores las variables que representan, el grosor de la cintura, el grosor promedio de la parte más delgada de ambas muñecas, el grosor promedio de ambos muslos bajo el pliegue del glúteo, la altura y el grosor promedio de la parte más ancha de ambas pantorrillas, este obtiene un r d}cuadrado de 0,9606 por lo cual podemos afirmar que el modelo múltiple reduce en un 96,05% la varianza aleatoria respecto al modelo nulo, además de que esta reducción es significativa puesto que el valor p obtenido del modelo es mucho menor al nivel de significancia establecido.

```{r generalidad}
g1 = residualPlots(modeloSimple, type = "rstandard", id=list(method = "r", n = 3, cex = 0.7, location = "lr"),
                   col = "steelblue", pch = 20, col.quad = c("steelblue", "red"))
print(durbinWatsonTest(modeloSimple))

mm1 = marginalModelPlots(modeloSimple, sd = TRUE, id=list(method = "r", n = 3, cex = 0.7, location = "lr"),
                   col = "steelblue", pch = 20, col.quad = c("steelblue", "red"))
```
A partir del gráfico de residuos podemos observar que se cumplen todas las condiciones para aplicar el método de mínimos cuadrados, puesto que podemos observar que los valores se distribuyen aleatoriamente al rededor de la linea del valor 0, forman una banda horizontal en torno a la linea del valor 0, los residuos no forman un patrón reconocible y por lo tanto tampoco hay valores que se alejan del patr+on que forma los demás. Cabe resaltar que el valor p obtenido por la prueba de no aditividad es significativo puesto que es menor al nivel de significancia, por lo cual podriamos afirmar que existe evidencia para creer que la relación entre el grosor de la cintura y el peso no es lineal, además la prueba de Durbin-Watson reporta un valor p de 0,102, el cual es mayor al nivel de significancia por lo cual fallamos en rechazar la hipótesis nula de la prueba y concluimos que los datos no podemos descartar que las observaciones no presentan autocorrelación
```{r comprobar modelo}
rmse_modeloSimple = sqrt(mean(resid(modeloSimple) ** 2))

prediccionesSimple = predict(modeloSimple, datos_30)

errorSimple = datos_30[["Weight"]] - prediccionesSimple
rmse_prueba = sqrt(mean(errorSimple) ** 2)
```

